{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import functions as fn, Row\n",
    "spark = SparkSession.builder.appName('project_model').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "     load(\"ml_df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# points in training:  126066\n",
      "# points in validation:  63009\n",
      "# points in testing:  21138\n"
     ]
    }
   ],
   "source": [
    "training_df, validation_df, testing_df = df.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "print(\"# points in training: \", training_df.count())\n",
    "print(\"# points in validation: \", validation_df.count())\n",
    "print(\"# points in testing: \", testing_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = df.schema.names\n",
    "variables.remove('TARGET')\n",
    "len(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 AUC Val: 0.6957276289469746\n",
      "Model 2 AUC Test: 0.696638207247368\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\n",
    "from pyspark import sql\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "stages_2 = []\n",
    "\n",
    "assemblerInputs = variables\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol='features')\n",
    "stages_2 += [assembler]\n",
    "\n",
    "scaler = StandardScaler(withMean=True, inputCol='features', outputCol='zfeatures')\n",
    "stages_2 += [scaler]\n",
    "\n",
    "pca = PCA(k=22, inputCol=\"zfeatures\", outputCol=\"pcaFeatures\")\n",
    "stages_2+=[pca]\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr2 = LogisticRegression(labelCol=\"TARGET\", featuresCol=\"pcaFeatures\", maxIter=100, elasticNetParam=.8)\n",
    "stages_2 += [lr2]\n",
    "\n",
    "lr_pipe_2 = Pipeline(stages = stages_2).fit(training_df)\n",
    "\n",
    "evaluator2 = evaluation.BinaryClassificationEvaluator(labelCol='TARGET')\n",
    "print(\"Model 2 AUC Val:\", evaluator2.evaluate(lr_pipe_2.transform(validation_df)))\n",
    "print(\"Model 2 AUC Test:\", evaluator2.evaluate(lr_pipe_2.transform(testing_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\n",
    "from pyspark import sql\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "\n",
    "stages_2 = []\n",
    "\n",
    "assemblerInputs = variables\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol='features')\n",
    "stages_2 += [assembler]\n",
    "\n",
    "scaler = StandardScaler(withMean=True, inputCol='features', outputCol='zfeatures')\n",
    "stages_2 += [scaler]\n",
    "\n",
    "pca = PCA(k=22, inputCol=\"zfeatures\", outputCol=\"pcaFeatures\")\n",
    "stages_2+=[pca]\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"pcaFeatures\", numTrees=100)\n",
    "stages_2 += [rf]\n",
    "\n",
    "lr_pipe_2 = Pipeline(stages = stages_2).fit(training_df)\n",
    "\n",
    "evaluator2 = evaluation.BinaryClassificationEvaluator(labelCol='TARGET')\n",
    "print(\"Model 2 AUC Val:\", evaluator2.evaluate(lr_pipe_2.transform(validation_df)))\n",
    "print(\"Model 2 AUC Test:\", evaluator2.evaluate(lr_pipe_2.transform(testing_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\n",
    "from pyspark import sql\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "stages_2 = []\n",
    "\n",
    "assemblerInputs = variables\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol='features')\n",
    "stages_2 += [assembler]\n",
    "\n",
    "scaler = StandardScaler(withMean=True, inputCol='features', outputCol='zfeatures')\n",
    "stages_2 += [scaler]\n",
    "\n",
    "pca = PCA(k=22, inputCol=\"zfeatures\", outputCol=\"pcaFeatures\")\n",
    "stages_2+=[pca]\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"TARGET\", featuresCol=\"pcaFeatures\", maxIter=100, maxBins=50,maxDepth=10)\n",
    "stages_2+=[gbt]\n",
    "\n",
    "lr_pipe_2 = Pipeline(stages = stages_2).fit(training_df)\n",
    "\n",
    "evaluator2 = evaluation.BinaryClassificationEvaluator(labelCol='TARGET')\n",
    "print(\"Model 2 AUC Val:\", evaluator2.evaluate(lr_pipe_2.transform(validation_df)))\n",
    "print(\"Model 2 AUC Test:\", evaluator2.evaluate(lr_pipe_2.transform(testing_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\n",
    "from pyspark import sql\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "stages_2 = []\n",
    "\n",
    "assemblerInputs = variables\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol='features')\n",
    "stages_2 += [assembler]\n",
    "\n",
    "scaler = StandardScaler(withMean=True, inputCol='features', outputCol='zfeatures')\n",
    "stages_2 += [scaler]\n",
    "\n",
    "pca = PCA(k=22, inputCol=\"zfeatures\", outputCol=\"pcaFeatures\")\n",
    "stages_2+=[pca]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [170, 30, 20, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(featuresCol='pcaFeatures',labelCol='TARGET',maxIter=100,\n",
    "                                         layers=layers, blockSize=128, seed=1234)\n",
    "stages_2 += [trainer]\n",
    "\n",
    "lr_pipe_2 = Pipeline(stages = stages_2).fit(training_df)\n",
    "\n",
    "evaluator2 = evaluation.BinaryClassificationEvaluator(labelCol='TARGET')\n",
    "print(\"Model 2 AUC Val:\", evaluator2.evaluate(lr_pipe_2.transform(validation_df)))\n",
    "print(\"Model 2 AUC Test:\", evaluator2.evaluate(lr_pipe_2.transform(testing_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = lr_pipe_3.stages[2].pc.toArray()\n",
    "print(principal_components)\n",
    "pca=list(zip(variables, \n",
    "         principal_components[:, 0], principal_components[:, 1], principal_components[:, 2],\n",
    "        principal_components[:, 3], principal_components[:, 4], principal_components[:, 5],\n",
    "        principal_components[:, 6], principal_components[:, 7], principal_components[:, 8],\n",
    "        principal_components[:, 9]))\n",
    "import pandas as pd\n",
    "pca_df = pd.DataFrame(pca)\n",
    "pca_df.columns = ['feature','pca_0','pca_1','pca_2','pca_3','pca_4','pca_5','pca_6',\n",
    "                 'pca_7','pca_8','pca_9']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
